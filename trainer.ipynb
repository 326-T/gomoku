{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c406823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726a17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.model import FCNN, FCNN_controller\n",
    "from modules.env import Gomoku, Observer, Othello, OthelloObserver\n",
    "from modules.agent import FNAgent, DNAgent, DNQAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9b6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", [\"s\", \"a\", \"r\", \"n_s\", \"n_o\", \"d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8151ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, buffer_size=1024, batch_size=128, epoch=2, gamma=0.9):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.gamma = gamma\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.logger = Logger()\n",
    "\n",
    "    @property\n",
    "    def trainer_name(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        snaked = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", class_name)\n",
    "        snaked = re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", snaked).lower()\n",
    "        snaked = snaked.replace(\"_trainer\", \"\")\n",
    "        return snaked\n",
    "\n",
    "    def train_loop(self, env, agent, opponent, episode=50000):\n",
    "        self.experiences = deque(maxlen=self.buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "\n",
    "        for i in range(episode):\n",
    "            state, options = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            # 先攻か後攻か\n",
    "            if np.random.random() < 0.5:\n",
    "                agent.turn = -1\n",
    "                opponent.turn = 1\n",
    "                opponent_action = opponent.policy(state, options)\n",
    "                state, reward, done, options = env.step(opponent.turn, opponent_action)\n",
    "                \n",
    "            else:\n",
    "                agent.turn = 1\n",
    "                opponent.turn = -1\n",
    "            \n",
    "            while not done:\n",
    "                if len(options) != 0:\n",
    "                    action = agent.policy(state, options)\n",
    "                    next_state, reward, done, next_options = env.step(agent.turn, action)\n",
    "                    acted = True\n",
    "                else:\n",
    "                    next_state, reward, done, next_options = env.step(agent.turn, None)\n",
    "\n",
    "                if not done:\n",
    "                    if len(next_options) != 0:\n",
    "                        opponent_action = opponent.policy(next_state, next_options)\n",
    "                        next_state, reward, done, next_options = env.step(opponent.turn, opponent_action)\n",
    "                    else: \n",
    "                        next_state, reward, done, next_options = env.step(opponent.turn, None)\n",
    "\n",
    "                    reward = env.reward(agent.turn)\n",
    "    \n",
    "                if acted and len(next_options) != 0:\n",
    "                    e = Experience(state, action, reward, next_state, next_options, done)\n",
    "                    self.experiences.append(e)\n",
    "                    self.training_count += 1\n",
    "                    acted = False\n",
    "    \n",
    "                if self.training_count == self.buffer_size:\n",
    "                    loss = agent.replay(self.experiences, self.gamma, self.batch_size, self.epoch)\n",
    "                    self.logger.loss.append(loss)\n",
    "                    self.training_count = 0\n",
    "\n",
    "                state = next_state\n",
    "                options = next_options\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            #     env.render()\n",
    "            self.logger.reward.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591ebc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss = []\n",
    "        self.reward = []\n",
    "        \n",
    "    def render(self, path=\"data/reward_loss.png\"):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        self._plot_loss(axes[0])\n",
    "        self._plot_reward(axes[1])\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    def _plot_loss(self, ax):\n",
    "        ax.plot(range(len(self.loss)), self.loss)\n",
    "        \n",
    "    def _plot_reward(self, ax, window=1024):\n",
    "        ax.plot(range(len(self.reward) - window + 1), self._moving_average(self.reward, window))\n",
    "    \n",
    "    def _moving_average(self, x, w):\n",
    "        return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f61858a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    env = Observer.load(Gomoku(3))\n",
    "    trainer = Trainer()\n",
    "    model = FCNN_controller(FCNN(env.dim_state+2, 1))\n",
    "    agent = FNAgent.load(model, env.dim_action)\n",
    "    opponent = FNAgent(0)\n",
    "    trainer.train_loop(env, agent, opponent)\n",
    "    agent.model.save_weight(\"data/gomoku/fn/0_model_fcnn\")\n",
    "    trainer.logger.render(\"data/gomoku/fn/0_reward_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6cede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_more(generation=1):\n",
    "    print(generation)\n",
    "    env = Observer.load(Gomoku(3))\n",
    "    trainer = Trainer()\n",
    "    \n",
    "    agent_model = FCNN_controller(FCNN(env.dim_state+2, 1))\n",
    "    agent_model.load_weight(\"data/gomoku/fn/\"+str(i)+\"_model_fcnn\")\n",
    "    agent = FNAgent.load(agent_model, env.dim_action)\n",
    "    \n",
    "    opponent_model = FCNN_controller(FCNN(env.dim_state+2, 1))\n",
    "    opponent_model.load_weight(\"data/gomoku/fn/\"+str(i)+\"_model_fcnn\")\n",
    "    opponent = FNAgent.load(opponent_model, env.dim_action)\n",
    "    \n",
    "    trainer.train_loop(env, agent, opponent)\n",
    "    \n",
    "    agent.model.save_weight(\"data/gomoku/fn/\"+str(i+1)+\"_model_fcnn\")\n",
    "    trainer.logger.render(\"data/gomoku/fn/\"+str(i+1)+\"_reward_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12491d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DNN():\n",
    "    env = Observer.load(Gomoku(3))\n",
    "    trainer = Trainer()\n",
    "    model = FCNN_controller(FCNN(env.dim_state+2, 1))\n",
    "    target = FCNN_controller(FCNN(env.dim_state+2, 1))\n",
    "    agent = DNQAgent.load(model, target, env.dim_action)\n",
    "    opponent = DNQAgent(0)\n",
    "    trainer.train_loop(env, agent, opponent)\n",
    "    agent.model.save_weight(\"data/gomoku/dnn/0_model_fcnn\")\n",
    "    trainer.logger.render(\"data/gomoku/dnn/0_reward_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1b718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_more_DNN(generation=1):\n",
    "    env = Observer.load(Gomoku(3))\n",
    "    trainer = Trainer()\n",
    "    \n",
    "    agent_model = FCNN_controller(FCNN(env.dim_state+2, 1))\n",
    "    agent_target = FCNN_controller(FCNN(env.dim_state+2, 1))\n",
    "    agent_model.load_weight(\"data/gomoku/dnn/\"+str(i)+\"_model_fcnn\")\n",
    "    agent_target.load_weight(\"data/gomoku/dnn/\"+str(i)+\"_model_fcnn\")\n",
    "    agent = DNQAgent.load(agent_model, agent_target, env.dim_action)\n",
    "    \n",
    "    opponent_model = FCNN_controller(FCNN(env.dim_state+2, 1))\n",
    "    opponent_model.load_weight(\"data/gomoku/dnn/\"+str(i)+\"_model_fcnn\")\n",
    "    opponent = DNQAgent.load(opponent_model, None, env.dim_action)\n",
    "    \n",
    "    trainer.train_loop(env, agent, opponent)\n",
    "    agent.model.save_weight(\"data/gomoku/dnn/\"+str(i+1)+\"_model_fcnn\")\n",
    "    trainer.logger.render(\"data/gomoku/dnn/\"+str(i+1)+\"_reward_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95b69dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Othello_DNN():\n",
    "    env = OthelloObserver.load(Othello())\n",
    "    trainer = Trainer()\n",
    "    model = FCNN_controller(FCNN(env.dim_state+2, 1, hidden_shape=[100, 50]))\n",
    "    target = FCNN_controller(FCNN(env.dim_state+2, 1, hidden_shape=[100, 50]))\n",
    "    agent = DNQAgent.load(model, target, env.dim_action)\n",
    "    opponent = DNQAgent(0)\n",
    "    trainer.train_loop(env, agent, opponent)\n",
    "    agent.model.save_weight(\"data/othello/dnn/0_model_fcnn\")\n",
    "    trainer.logger.render(\"data/othello/dnn/0_reward_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e67737a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_more_Othello_DNN(generation=1):\n",
    "    env = OthelloObserver.load(Othello())\n",
    "    trainer = Trainer()\n",
    "    \n",
    "    agent_model = FCNN_controller(FCNN(env.dim_state+2, 1, hidden_shape=[100, 50]))\n",
    "    agent_target = FCNN_controller(FCNN(env.dim_state+2, 1, hidden_shape=[100, 50]))\n",
    "    agent_model.load_weight(\"data/othello/dnn/\"+str(i)+\"_model_fcnn\")\n",
    "    agent_target.load_weight(\"data/othello/dnn/\"+str(i)+\"_model_fcnn\")\n",
    "    agent = DNQAgent.load(agent_model, agent_target, env.dim_action)\n",
    "    \n",
    "    opponent_model = FCNN_controller(FCNN(env.dim_state+2, 1, hidden_shape=[100, 50]))\n",
    "    opponent_model.load_weight(\"data/othello/dnn/\"+str(i)+\"_model_fcnn\")\n",
    "    opponent = DNQAgent.load(opponent_model, None, env.dim_action)\n",
    "    \n",
    "    trainer.train_loop(env, agent, opponent)\n",
    "    agent.model.save_weight(\"data/othello/dnn/\"+str(i+1)+\"_model_fcnn\")\n",
    "    trainer.logger.render(\"data/othello/dnn/\"+str(i+1)+\"_reward_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c660556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_Othello_DNN()\n",
    "    for i in range(10):\n",
    "        train_more_Othello_DNN(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b54b9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
