{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c7801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec4fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNAgent():\n",
    "\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.model = None\n",
    "        self.nn_controller = None\n",
    "        self.estimate_probs = False\n",
    "        self.initialized = False\n",
    "        self.color = 1\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model, epsilon=0.1):\n",
    "        agent = cls(epsilon)\n",
    "        agent.model = model\n",
    "        agent.initialized = True\n",
    "        return agent\n",
    "\n",
    "    def estimate(self, state):\n",
    "        x = self._append_turn(state)\n",
    "        return self.model.predict(x).detach().numpy()\n",
    "        \n",
    "    def policy(self, state, options):\n",
    "        if np.random.random() < self.epsilon or not self.initialized:\n",
    "            return np.random.choice(options, 1)[0]\n",
    "        else:\n",
    "            estimates = self.estimate(state)[0]\n",
    "            return self._choice_from_options(estimates, options)\n",
    "\n",
    "    def replay(self, experiences, gamma=0.9, batch_size=128, max_epoch=2):\n",
    "        states = np.vstack([e.s for e in experiences])\n",
    "        next_states = np.vstack([e.n_s for e in experiences])\n",
    "\n",
    "        estimateds = self.estimate(states)\n",
    "        future = self.estimate(next_states)\n",
    "\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.r\n",
    "            if not e.d:\n",
    "                reward += gamma * np.max(future[i])\n",
    "            estimateds[i][e.a] = reward\n",
    "\n",
    "        x = self._append_turn(states)\n",
    "        loss = self.model.train(x , estimateds, batch_size, max_epoch)\n",
    "        return loss\n",
    "    \n",
    "    def _choice_from_options(self, estimates, options):\n",
    "        index = np.argmax(estimates[options])\n",
    "        return options[index]\n",
    "    \n",
    "    def _append_turn(self, state):\n",
    "        if state.ndim == 1:\n",
    "            return np.block([state, self.color])\n",
    "        else:\n",
    "            return np.block([state, np.ones([state.shape[0], 1]) * self.color])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNQAgent(FNAgent):\n",
    "\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__(epsilon)\n",
    "        self.max_action = 1\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model, max_action, epsilon=0.1):\n",
    "        agent = super().load(model, epsilon)\n",
    "        agent.max_action = max_action\n",
    "        return agent\n",
    "\n",
    "    def estimate(self, state, action):\n",
    "        x = self._append_turn_action(state, action)\n",
    "        return self.model.predict(x).detach().numpy()\n",
    "        \n",
    "    def policy(self, state, options):\n",
    "        if np.random.random() < self.epsilon or not self.initialized:\n",
    "            return np.random.choice(options, 1)[0]\n",
    "        else:\n",
    "            max_id = np.argmax([self.estimate(state, option)[0] for option in options])\n",
    "            return options[max_id]\n",
    "\n",
    "    def replay(self, experiences, gamma=0.9, batch_size=128, max_epoch=2):\n",
    "        states = np.array([e.s for e in experiences])\n",
    "        actions = np.array([e.a for e in experiences])\n",
    "\n",
    "        estimateds = np.zeros([states.shape[0], 1])\n",
    "\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.r\n",
    "            if not e.d:\n",
    "                reward += gamma * np.max([self.estimate(e.n_s, o) for o in e.n_o])\n",
    "            estimateds[i][0] = reward\n",
    "\n",
    "        x = self._append_turn_action(states, actions)\n",
    "        loss = self.model.train(x, estimateds, batch_size, max_epoch)\n",
    "        return loss\n",
    "    \n",
    "    def _append_turn_action(self, state, action):\n",
    "        if state.ndim == 1:\n",
    "            return np.block([state, self.color, action / self.max_action])\n",
    "        else:\n",
    "            return np.block([state, np.ones([state.shape[0], 1]) * self.color, action.reshape(-1, 1) / self.max_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f755872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNAgent():\n",
    "\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.model = None\n",
    "        self.target = None\n",
    "        self.nn_controller = None\n",
    "        self.estimate_probs = False\n",
    "        self.initialized = False\n",
    "        self.color = 1\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model, target, max_action, epsilon=0.1):\n",
    "        agent = cls(epsilon)\n",
    "        agent.model = model\n",
    "        agent.target = target\n",
    "        agent.max_action = max_action\n",
    "        agent.initialized = True\n",
    "        return agent\n",
    "\n",
    "    def estimate(self, state, use_target=False):\n",
    "        x = self._append_turn(state)\n",
    "        if use_target:\n",
    "            return self.target.predict(x).detach().numpy()\n",
    "        return self.model.predict(x).detach().numpy()\n",
    "        \n",
    "    def policy(self, state, options):\n",
    "        if np.random.random() < self.epsilon or not self.initialized:\n",
    "            return np.random.choice(options, 1)[0]\n",
    "        else:\n",
    "            estimates = self.estimate(state)[0]\n",
    "            return self._choice_from_options(estimates, options)\n",
    "\n",
    "    def replay(self, experiences, gamma=0.9, batch_size=128, max_epoch=2):\n",
    "        loss = 0\n",
    "        for epoch in range(max_epoch):\n",
    "            dataset = NumpyDataset(experiences, batch_size)\n",
    "            for es in dataset:\n",
    "                states = np.array([e.s for e in es])\n",
    "                next_states = np.array([e.n_s for e in es])\n",
    "                actions = np.array([e.a for e in es])\n",
    "\n",
    "                estimateds = self.estimate(states)\n",
    "                future = self.estimate(next_states, use_target=True)\n",
    "\n",
    "                for i, e in enumerate(es):\n",
    "                    reward = e.r\n",
    "                    if not e.d:\n",
    "                        reward += gamma * np.max(future[i])\n",
    "                    estimateds[i][e.a] = reward\n",
    "\n",
    "                x = self._append_turn(states)\n",
    "                loss += self.model.train(x, estimateds, batch_size, 1)\n",
    "\n",
    "        self._hard_copy()\n",
    "        return loss / (len(dataset) * max_epoch)\n",
    "    \n",
    "    def _choice_from_options(self, estimates, options):\n",
    "        index = np.argmax(estimates[options])\n",
    "        return options[index]\n",
    "    \n",
    "    def _append_turn(self, state):\n",
    "        if state.ndim == 1:\n",
    "            return np.block([state, self.color])\n",
    "        else:\n",
    "            return np.block([state, np.ones([state.shape[0], 1]) * self.color])\n",
    "\n",
    "    def _hard_copy(self):\n",
    "        self.target.model.load_state_dict(self.model.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ca811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNQAgent(DNAgent):\n",
    "\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__(epsilon)\n",
    "        self.max_action = 1\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model, target, max_action, epsilon=0.1):\n",
    "        agent = super().load(model, target, epsilon)\n",
    "        agent.max_action = max_action\n",
    "        return agent\n",
    "\n",
    "    def estimate(self, state, action, use_target=False):\n",
    "        x = self._append_turn_action(state, action)\n",
    "        if use_target:\n",
    "            return self.target.predict(x).detach().numpy()\n",
    "        return self.model.predict(x).detach().numpy()\n",
    "        \n",
    "    def policy(self, state, options):\n",
    "        if np.random.random() < self.epsilon or not self.initialized:\n",
    "            return np.random.choice(options, 1)[0]\n",
    "        else:\n",
    "            max_id = np.argmax([self.estimate(state, option)[0] for option in options])\n",
    "            return options[max_id]\n",
    "\n",
    "    def replay(self, experiences, gamma=0.9, batch_size=128, max_epoch=2):\n",
    "        loss = 0\n",
    "        for i in range(max_epoch):\n",
    "            dataset = NumpyDataset(experiences, batch_size)\n",
    "            for es in dataset:\n",
    "                states = np.array([e.s for e in es])\n",
    "                actions = np.array([e.a for e in es])\n",
    "\n",
    "                estimateds = np.zeros([states.shape[0], 1])\n",
    "\n",
    "                for i, e in enumerate(es):\n",
    "                    reward = e.r\n",
    "                    if not e.d:\n",
    "                        reward += gamma * np.max([self.estimate(e.n_s, o, use_target=True) for o in e.n_o])\n",
    "                    estimateds[i][0] = reward\n",
    "\n",
    "                x = self._append_turn_action(states, actions)\n",
    "                loss += self.model.train(x, estimateds, batch_size, 1)\n",
    "\n",
    "        self._hard_copy()\n",
    "        return loss / (len(dataset) * max_epoch)\n",
    "    \n",
    "    def _append_turn_action(self, state, action):\n",
    "        if state.ndim == 1:\n",
    "            return np.block([state, self.color, action / self.max_action])\n",
    "        else:\n",
    "            return np.block([state, np.ones([state.shape[0], 1]) * self.color, action.reshape(-1, 1) / self.max_action])\n",
    "\n",
    "    def _hard_copy(self):\n",
    "        self.target.model.load_state_dict(self.model.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset():\n",
    "\n",
    "    def __init__(self, x, batch_size):\n",
    "        self.x = x.copy()\n",
    "        if type(x) is deque:\n",
    "            self.x = list(self.x)\n",
    "            \n",
    "        rng = np.random.default_rng()\n",
    "        rng.shuffle(self.x, axis=0)\n",
    "        self.batch_size = batch_size\n",
    "        self.size = (len(x) - 1) // batch_size + 1\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        if id < 0 or self.__len__() <= id:\n",
    "            raise IndexError\n",
    "        max_id = min(self.batch_size * (id+1), len(self.x))\n",
    "        return self.x[self.batch_size * id : max_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ec421",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    a = np.arange(12).reshape(3,4).tolist()\n",
    "    for i in NumpyDataset(a, 2):\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
